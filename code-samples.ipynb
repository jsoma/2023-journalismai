{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a5a7e98",
   "metadata": {},
   "source": [
    "## Installing prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c41035",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --upgrade torch transformers \\\n",
    "    sentence-transformers sentencepiece \\\n",
    "    protobuf==3.20 pystemmer eli5 \\\n",
    "    openai-whisper scikit-learn chromadb \\\n",
    "    openai langchain==0.0.198"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "efdfce2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "def set_css():\n",
    "    display(HTML('''\n",
    "    <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "    </style>\n",
    "    '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a5f97c",
   "metadata": {},
   "source": [
    "## Downloading our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc2cb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/jsoma/2023-journalismai/main/book.txt\n",
    "!wget https://raw.githubusercontent.com/jsoma/2023-journalismai/main/folktale.txt\n",
    "!wget https://raw.githubusercontent.com/jsoma/2023-journalismai/main/wapo-reviews-marked.csv\n",
    "!wget https://raw.githubusercontent.com/jsoma/2023-journalismai/main/nytimes-story.txt\n",
    "!wget https://raw.githubusercontent.com/jsoma/2023-journalismai/main/6313.mp3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b1965d",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n",
    "\n",
    "Sentiment analysis is a judge of whether a text is **positive** or **negative**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f272850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "data = [\"I love sandwiches\"]\n",
    "sentiment_pipeline(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79ebdc7",
   "metadata": {},
   "source": [
    "Oh, it looks like we should [specify a model?](https://huggingface.co/models) Let's just use the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efecee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipeline = pipeline(\"sentiment-analysis\",\n",
    "                             model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "data = [\"I love sandwiches\"]\n",
    "sentiment_pipeline(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9463fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipeline = pipeline(\"sentiment-analysis\",\n",
    "                             model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "data = [\"j'adore les sandwichs\"]\n",
    "sentiment_pipeline(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f408923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipeline = pipeline(\"sentiment-analysis\",\n",
    "                             model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "data = [\"я люблю бутерброды\"]\n",
    "sentiment_pipeline(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f293cf16",
   "metadata": {},
   "source": [
    "If we want to try another one, we can look at [the most popular ones](https://huggingface.co/models?pipeline_tag=text-classification&sort=downloads&search=sentiment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61335b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipeline = pipeline(\"sentiment-analysis\",\n",
    "                             model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n",
    "data = [\"я люблю бутерброды\"]\n",
    "sentiment_pipeline(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0be69d",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "**Classification** is a classic problem in investigative journalism.\n",
    "\n",
    "You have a lot of documents: how do you find the ones you're interested in?\n",
    "\n",
    "- Atlanta Journal-Constitution: [Doctors & Sex Abuse: Still forgiven](https://doctors.ajc.com/)\n",
    "- Washington Post: [Apple says its App Store is ‘a safe and trusted place.’ We found 1,500 reports of unwanted sexual behavior on six apps, some targeting minors.](https://www.washingtonpost.com/technology/2019/11/22/apple-says-its-app-store-is-safe-trusted-place-we-found-reports-unwanted-sexual-behavior-six-apps-some-targeting-minors/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d01595b",
   "metadata": {},
   "source": [
    "### The old approach\n",
    "\n",
    "Historically, you labeled a subset, then used a **machine learning algorithm** that scored the rest of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ccd15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", 300)\n",
    "\n",
    "df = pd.read_csv(\"wapo-reviews-marked.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003573f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "known = df[df.sexual.notna()].copy()\n",
    "unknown = df[df.sexual.isna()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c20c634",
   "metadata": {},
   "outputs": [],
   "source": [
    "known.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5578c746",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import Stemmer\n",
    "\n",
    "stemmer = Stemmer.Stemmer('en')\n",
    "\n",
    "analyzer = TfidfVectorizer().build_analyzer()\n",
    "\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: stemmer.stemWords(analyzer(doc))\n",
    "\n",
    "vectorizer = StemmedTfidfVectorizer(max_features=500, max_df=0.30)\n",
    "matrix = vectorizer.fit_transform(known.Review)\n",
    "\n",
    "words_df = pd.DataFrame(matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "words_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24fa941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "X = matrix\n",
    "y = known.sexual\n",
    "\n",
    "clf = LinearSVC(class_weight='balanced')\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8704ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.transform(unknown.Review)\n",
    "\n",
    "unknown['predicted'] = clf.predict(X)\n",
    "unknown['predicted_proba'] = clf.decision_function(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093dc22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fd26f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown[unknown.predicted == 1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63b7013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The top 1000 most likely creepy reviews\n",
    "\n",
    "creepy_df = unknown.sort_values(by='predicted_proba',\n",
    "                                ascending=False).head(1000)\n",
    "creepy_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2921c4c0",
   "metadata": {},
   "source": [
    "### Using a fine-tuned language model\n",
    "\n",
    "The modern update to this might use [HuggingFace AutoTrain](https://huggingface.co/autotrain) to create a custom model. It will (potentially) be more effective than your old-fashioned machine learning model, with fewer parameters to tweak.\n",
    "\n",
    "I trained a small model called [creepy-wapo](https://huggingface.co/wendys-llc/creepy-wapo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65235592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "creepy_pipeline = pipeline(model=\"wendys-llc/creepy-wapo\")\n",
    "data = [\n",
    "    \"I love the app, talking to people is fun\",\n",
    "    \"Be careful talking to men, they all want nudes :(\"\n",
    "]\n",
    "\n",
    "creepy_pipeline(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856b9946",
   "metadata": {},
   "source": [
    "### Using zero-shot classification with GPT\n",
    "\n",
    "The *most* advanced method is to [just ask GPT](https://chat.openai.com/). This is called zero-shot classification because it doesn't need any examples!\n",
    "\n",
    "While you could [just use ChatGPT like the Marshall Project did](https://generative-ai-newsroom.com/decoding-bureaucracy-5b0c1411171), using code is much faster and much more controllable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb058d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# You'll need your own OpenAI GPT API key! \n",
    "# https://platform.openai.com/apps\n",
    "API_KEY = \"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\"\n",
    "\n",
    "# Faster/cheaper\n",
    "MODEL = 'gpt-3.5-turbo'\n",
    "\n",
    "# Better results (I'm impatient, so we're using turbo!)\n",
    "# MODEL = 'gpt-4'\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=API_KEY, model_name=MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42df2a79",
   "metadata": {},
   "source": [
    "Here is an example of talking to GPT using Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b57c4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.predict(\"Give me a recipe for chocolate-chip cookies\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28b3889",
   "metadata": {},
   "source": [
    "Here is an example of zero-shot classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae503ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Categorize the following text as being about ENVIRONMENT, GUN CONTROL,\n",
    "or IMMIGRATION. Respond with only the category.\n",
    "\n",
    "Text: A Bill to Regulate the Sulfur Emissions of Coal-Fired Energy\n",
    "Plants in the State of New York.\n",
    "\"\"\"\n",
    "\n",
    "response = llm.predict(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c99f213",
   "metadata": {},
   "source": [
    "Normally you would use this for a whole lot of different bills, so it would be best to design a template that you can fill in text for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbcf159",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Categorize the following text as being about ENVIRONMENT, GUN CONTROL,\n",
    "or IMMIGRATION. Respond with only the category.\n",
    "\n",
    "Text: {bill_text}\n",
    "\"\"\"\n",
    "\n",
    "bills = [\n",
    "    \"A Bill to Allow Additional Refugees In Upstate New York\",\n",
    "    \"A Bill to Close Down Coal-fired Power Plants\",\n",
    "    \"A Bill to Banning Assault Rifles at Public Events\"\n",
    "]\n",
    "\n",
    "for bill in bills:\n",
    "    prompt = template.format(bill_text=bill)\n",
    "    response = llm.predict(prompt)\n",
    "    print(bill, \"is\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ff9e2f",
   "metadata": {},
   "source": [
    "## Summarization\n",
    "\n",
    "Let's say we wanted to summarize [this story from the NYT](https://www.nytimes.com/2023/08/08/business/china-youth-unemployment.html) about youth unemployment in China. We have a few options!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5ae358",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"nytimes-story.txt\").read()\n",
    "text[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b591181",
   "metadata": {},
   "source": [
    "### Using a Hugging Face model to summarize\n",
    "\n",
    "Using a Hugging Face model is free, fast and private. For example, we can use [this model originally created by Facebook](https://huggingface.co/facebook/bart-large-cnn), which is a [popular model for summarization](https://huggingface.co/models?pipeline_tag=summarization&sort=trending)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fd5184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6682810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can't send the whole text! We're only sending the first half.\n",
    "\n",
    "result = summarizer(text[:4000], max_length=300, min_length=30)\n",
    "print(result['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b3e111",
   "metadata": {},
   "source": [
    "### Using GPT to summarize\n",
    "\n",
    "On the other hand, GPT results might be more expensive (and less private), but the quality will certainly be much higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4defdb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Write a concise summary of the following text.\n",
    "\n",
    "TEXT: {story_text}\n",
    "\"\"\"\n",
    "\n",
    "prompt = template.format(story_text=text)\n",
    "response = llm.predict(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be26551",
   "metadata": {},
   "source": [
    "We can use **prompt engineering** to customize our results. You can learn more at [Prompt Engineering for Developers](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33177064",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Write a concise summary of the following text in bullet-point format.\n",
    "Address topics as action items, and assume the reader knows the basic\n",
    "facts of the situation.\n",
    "\n",
    "TEXT: {story_text}\n",
    "\"\"\"\n",
    "\n",
    "prompt = template.format(story_text=text)\n",
    "response = llm.predict(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aea4c1",
   "metadata": {},
   "source": [
    "### Summarizing longer texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935a6fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"folktale.txt\").read()\n",
    "text[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f2cd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Write a concise summary of the following text.\n",
    "\n",
    "TEXT: {story_text}\n",
    "\"\"\"\n",
    "\n",
    "# The below will give us an error\n",
    "# prompt = template.format(story_text=text)\n",
    "# response = llm.predict(prompt)\n",
    "\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfce7b1",
   "metadata": {},
   "source": [
    "Instead, we need to split it up into several pieces and summarize them one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe45522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = TextLoader('folktale.txt', encoding='utf-8')\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2500)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c52c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "prompt_template = \"\"\"Write a concise summary of the following text.\n",
    "\n",
    "TEXT: {text}\n",
    "\n",
    "\n",
    "CONCISE SUMMARY IN ENGLISH:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "chain = load_summarize_chain(llm,\n",
    "                             chain_type=\"map_reduce\",\n",
    "                             return_intermediate_steps=True,\n",
    "                             map_prompt=PROMPT,\n",
    "                             combine_prompt=PROMPT)\n",
    "\n",
    "result = chain({\"input_documents\": docs}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1d0a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fba7c08",
   "metadata": {},
   "source": [
    "## Embeddings and semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3463fd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"cat\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings[0][:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856470d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367f88cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sentences = [\n",
    "    \"Molly ate a fish\",\n",
    "    \"Jen consumed a carp\",\n",
    "    \"I would like to sell you a house\",\n",
    "    \"Я пытаюсь купить дачу\", # I'm trying to buy a summer home\n",
    "    \"J'aimerais vous louer un grand appartement\", # I would like to rent a large apartment to you\n",
    "    \"This is a wonderful investment opportunity\",\n",
    "    \"Это прекрасная возможность для инвестиций\", # investment opportunity\n",
    "    \"C'est une merveilleuse opportunité d'investissement\", # investment opportunity\n",
    "    \"これは素晴らしい投資機会です\", # investment opportunity\n",
    "    \"野球はあなたが思うよりも面白いことがあります\", # baseball can be more interesting than you think\n",
    "    \"Baseball can be more interesting than you'd think\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692f2282",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c5594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute similarities exactly the same as we did before!\n",
    "similarities = cosine_similarity(embeddings)\n",
    "\n",
    "# Turn into a dataframe\n",
    "pd.DataFrame(similarities,\n",
    "            index=sentences,\n",
    "            columns=sentences) \\\n",
    "            .style \\\n",
    "            .background_gradient(axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f335f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')\n",
    "embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a3d443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute similarities exactly the same as we did before!\n",
    "similarities = cosine_similarity(embeddings)\n",
    "\n",
    "# Turn into a dataframe\n",
    "pd.DataFrame(similarities,\n",
    "            index=sentences,\n",
    "            columns=sentences) \\\n",
    "            .style \\\n",
    "            .background_gradient(axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47570515",
   "metadata": {},
   "source": [
    "### Searching across a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af7c82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = TextLoader('book.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e8eaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf1f7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name='paraphrase-multilingual-MiniLM-L12-v2')\n",
    "docsearch = Chroma.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bd8fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = embeddings.embed_documents([\"What did Zsuzska steal from the devil?\"])[0]\n",
    "len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926709a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7eb1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k=1 because we only want one result\n",
    "docsearch.similarity_search(\"What did Zsuzska steal from the devil?\", k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744356dd",
   "metadata": {},
   "source": [
    "## Document-based question-and-answer\n",
    "\n",
    "We can then use the related documents to answer questions. The example below sends the top few results to GPT along with our question. This is called **document-based question-and-answer with semantic search**. Be careful, though, it isn't perfect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e887e065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# You'll need your own OpenAI GPT API key!\n",
    "# https://platform.openai.com/apps\n",
    "API_KEY = \"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\"\n",
    "\n",
    "# Faster/cheaper\n",
    "MODEL = 'gpt-3.5-turbo'\n",
    "\n",
    "# Better results (I'm impatient, so we're using turbo!)\n",
    "# MODEL = 'gpt-4'\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=API_KEY, model_name=MODEL, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409e20f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acbb4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What did Zsuzska steal from the devil?\"\n",
    "\n",
    "result = qa.run(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667406fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What did Zsuzska steal from the devil? Be sure to name everything!\"\n",
    "\n",
    "result = qa.run(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988b30f0",
   "metadata": {},
   "source": [
    "## Transcription\n",
    "\n",
    "We can use [Whisper](https://github.com/openai/whisper) to transcribe audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb6ebdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51bda8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = whisper.load_model(\"tiny\")\n",
    "\n",
    "result = model.transcribe(\"6313.mp3\")\n",
    "result['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59c4c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "result = model.transcribe(\"6313.mp3\")\n",
    "result['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dd8c0a",
   "metadata": {},
   "source": [
    "## What do you want to try to do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97dbd29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
